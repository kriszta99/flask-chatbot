# -*- coding: utf-8 -*-
"""Chunkolasi_algoritmus_teszteles_mérés.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DB9g7xLSgVXEWzK6m_h5MheH7fj6GUAA
"""

!pip install --upgrade openai --quiet

import openai
print(openai.__version__)

def read_markdown_file(file_path):
    """
    Markdown fájl beolvasása és szöveg kinyerése
    """
    with open(file_path, 'r', encoding='utf-8') as file:
        markdown_content = file.read()

    return markdown_content

text_m = read_markdown_file('markdown_karrol_.md')

text_m

!pip install tiktoken

import tiktoken
print(tiktoken.get_encoding("cl100k_base"))

import openai
import numpy as np
openai.api_key = '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'

import numpy as np
def get_embedding(text: str, model="text-embedding-3-large") -> np.ndarray:
#def get_embedding(text: str, model="text-embedding-3-small") -> np.ndarray:
#def get_embedding(text: str, model="text-embedding-ada-002") -> np.ndarray:
    """Szöveg beágyazásának lekérése OpenAI modellel."""
    response = openai.embeddings.create(input=text, model=model)
    #embedding = response['data'][0]['embedding']
    # Az új API-ban a válasz egy objektum, nem közvetlenül szótár
    embedding = response.data[0].embedding
    return np.array(embedding)

import tiktoken

# Tokenek számolása
def count_tokens(text, encoder):
    return len(encoder.encode(text))

# Sorok feldolgozása
def split_lines(text):
    return text.split('\n')

# A szöveg chunkolása sorok szerint, figyelembe véve a # címeket és megtartva az aktuális chunk_id-t
def chunk_text_by_line_with_headers_to_embeding(text, max_tokens=384, encoding_name='cl100k_base'):
    encoder = tiktoken.get_encoding(encoding_name)

    # Bontsuk sorokra
    lines = split_lines(text)
    chunks, current_chunk, current_token_count = [], [], 0
    current_header = None  # Kezdetben nincs fejléc
    chunk_index = 1  # Chunk index kezdése
    current_chunk_id = None  # Kezdetben nincs chunk_id

    for line in lines:
        line = line.strip()
        if not line:  # Üres sorokat kihagyunk
            continue
        # Ha a sor egy új főcím (pl. #), akkor új chunkot indítunk
        if line.startswith('# '):  # Főcím, pl. # Főcím
            if current_chunk:  # Ha van már aktuális chunk, hozzáadjuk
                emmbeded_chunk = get_embedding(current_chunk)
                chunks.append({
                    "chunk_index": f"chunk_{chunk_index}",
                    "chunk_id": current_chunk_id,  # Az aktuális chunk_id hozzáadása
                    "header": current_header,
                    "text": '\n'.join(current_chunk),
                    "embedding": emmbeded_chunk,
                    "token_count": current_token_count
                })
                current_chunk, current_token_count = [], 0
                chunk_index += 1  # Új chunk, növeljük az indexet

            # Frissítjük a fejlécet és a chunk_id-t
            current_header = line
            current_chunk_id = f"chunk_{chunk_index}"

        # Ha nem főcím, akkor ugyanahhoz a chunkhoz rendeljük
        token_count = count_tokens(line, encoder)

        # Ha a sor túl hosszú, új chunkot kezdünk
        if token_count > max_tokens:
            if current_chunk:
                emmbeded_chunk = get_embedding(current_chunk)
                chunks.append({
                    "chunk_index": f"chunk_{chunk_index}",
                    "chunk_id": current_chunk_id,  # Az aktuális chunk_id hozzáadása
                    "header": current_header,
                    "text": '\n'.join(current_chunk),
                    "embedding": emmbeded_chunk,
                    "token_count": current_token_count
                })
                current_chunk, current_token_count = [], 0
            emmbeded_chunk = get_embedding(line)
            chunks.append({
                "chunk_index": f"chunk_{chunk_index}",
                "chunk_id": current_chunk_id,  # Az aktuális chunk_id hozzáadása
                "header": current_header,
                "text": line,
                "embedding": emmbeded_chunk,
                "token_count": token_count
            })
            chunk_index += 1
            continue

        # Ha nem fér bele, új chunk
        if current_token_count + token_count > max_tokens - 1:
            if current_chunk:
                emmbeded_chunk = get_embedding(current_chunk)
                chunks.append({
                    "chunk_index": f"chunk_{chunk_index}",
                    "chunk_id": current_chunk_id,  # Az aktuális chunk_id hozzáadása
                    "header": current_header,
                    "text": '\n'.join(current_chunk),
                    "embedding": emmbeded_chunk,
                    "token_count": current_token_count
                })
            current_chunk, current_token_count = [], 0
            chunk_index += 1

        # Hozzáadjuk a sort az aktuális chunkhoz
        current_chunk.append(line)
        current_token_count += token_count

    # Ha maradt még nem hozzáadott chunk, azt is hozzáadjuk
    if current_chunk:
        emmbeded_chunk = get_embedding(current_chunk)
        chunks.append({
            "chunk_index": f"chunk_{chunk_index}",
            "chunk_id": current_chunk_id,  # Az aktuális chunk_id hozzáadása
            "header": current_header,
            "text": '\n'.join(current_chunk),
            "embedding": emmbeded_chunk,
            "token_count": current_token_count
        })

    return chunks




# Például: A text_m2 változó legyen a szöveg, amit chunkolni szeretnél.
chunks = chunk_text_by_line_with_headers_to_embeding(text_m, max_tokens=256)
# Eredmény kiíratása
for i, chunk in enumerate(chunks):
    print(f"Chunk {i + 1}: (Index: {chunk['chunk_index']}, Chunk ID: {chunk['chunk_id']}, Chunk order: {chunk['order']})\n{chunk['header']} -> {chunk['text']}\n Embedding:{chunk['embedding']}... [Token count: {chunk['token_count']}]")

#Open Ai modell-re
import json
import tiktoken
from google.colab import files

# Tokenek számolása
def count_tokens(text, encoder):
    return len(encoder.encode(text))

# Markdown fájl beolvasása
def read_markdown_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

# Sorok feldolgozása
def split_lines(text):
    return text.split('\n')

# Chunkolás és JSON mentés
def chunk_text_by_line_with_headers_to_embedding(file_path, max_tokens=256, encoding_name='cl100k_base', start_chunk_id=0):
    encoder = tiktoken.get_encoding(encoding_name)
    text = read_markdown_file(file_path)
    lines = split_lines(text)
    base_name = file_path.split('.')[0]


    chunks, current_chunk, current_token_count = [], [], 0
    current_header = None
    chunk_index = 1
    header_chunk_id_counter = start_chunk_id  # csak fejléc váltáskor nő

    current_chunk_id = f"chunk_{header_chunk_id_counter}"

    for line in lines:
        line = line.strip()
        if not line:
            continue

        if line.startswith('# '):
            if current_chunk:
                embedded = get_embedding(current_chunk)
                chunks.append({
                    "chunk_index": f"chunk_{chunk_index}",
                    "chunk_id": current_chunk_id,
                    "header": current_header,
                    "text": '\n'.join(current_chunk),
                    "embedding": embedded.tolist(),
                    "token_count": current_token_count
                })
                chunk_index += 1
                current_chunk, current_token_count = [], 0

            current_header = line
            header_chunk_id_counter += 1
            current_chunk_id = f"chunk_{header_chunk_id_counter}"

        token_count = count_tokens(line, encoder)

        if token_count > max_tokens:
            if current_chunk:
                embedded = get_embedding(current_chunk)
                chunks.append({
                    "chunk_index": f"chunk_{chunk_index}",
                    "chunk_id": current_chunk_id,
                    "header": current_header,
                    "text": '\n'.join(current_chunk),
                    "embedding": embedded.tolist(),
                    "token_count": current_token_count
                })
                chunk_index += 1
                current_chunk, current_token_count = [], 0

            #Ebben az esetben kell csak maualisan beletenni listaba mert a tobbi helyen listakent van megadva
            embedded = get_embedding([line])
            chunks.append({
                "chunk_index": f"chunk_{chunk_index}",
                "chunk_id": current_chunk_id,
                "header": current_header,
                "text": line,
                "embedding":embedded.tolist(),
                "token_count": token_count
            })
            chunk_index += 1
            continue

        if current_token_count + token_count > max_tokens:
            if current_chunk:
                embedded = get_embedding(current_chunk)
                chunks.append({
                    "chunk_index": f"chunk_{chunk_index}",
                    "chunk_id": current_chunk_id,
                    "header": current_header,
                    "text": '\n'.join(current_chunk),
                    "embedding": embedded.tolist(),
                    "token_count": current_token_count
                })
                chunk_index += 1
                current_chunk, current_token_count = [], 0

        current_chunk.append(line)
        current_token_count += token_count

    if current_chunk:
        embedded = get_embedding(current_chunk)
        chunks.append({
            "chunk_index": f"chunk_{chunk_index}",
            "chunk_id": current_chunk_id,
            "header": current_header,
            "text": '\n'.join(current_chunk),
            "embedding": embedded.tolist(),
            "token_count": current_token_count
        })

    # JSON mentés
    output_filename = f"{base_name}{max_tokens}_small.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)

    print(f"Mentve: {output_filename}")
    files.download(output_filename)
    return chunks

chunk_text_by_line_with_headers_to_embedding('markdown_karrol_.md',256,'cl100k_base')

chunk_text_by_line_with_headers_to_embedding('markdown_karrol_.md',384,'cl100k_base')

chunk_text_by_line_with_headers_to_embedding('markdown_karrol_.md',512,'cl100k_base')

chunk_text_by_line_with_headers_to_embedding('markdown_karrol_.md',768,'cl100k_base')

chunk_text_by_line_with_headers_to_embedding('markdown_karrol_.md',1024,'cl100k_base')

chunk_text_by_line_with_headers_to_embedding('markdown_karrol_.md',2048,'cl100k_base')

import time
import json
import numpy as np
import openai
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from google.colab import files


# OpenAI API kulcs beállítása
openai.api_key = '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'
# Szöveg beágyazásának lekérése OpenAI modellel
def get_embedding(text: str, model="text-embedding-ada-002") -> np.ndarray:
#def get_embedding(text: str, model="text-embedding-3-large") -> np.ndarray:
#def get_embedding(text: str, model="text-embedding-3-small") -> np.ndarray:
    response = openai.embeddings.create(input=text, model=model)
    # Az új API-ban a válasz egy objektum, nem közvetlenül szótár
    embedding = response.data[0].embedding
    return np.array(embedding)

# Kérdések és helyes válaszok
questions = [
    "Mikor nyilt meg a C épület a Capmpuson?",
    "Ki a tanulmányi és kredit bizottság elnöke?",
    "Mikor van nyitva a könyvtár vakációban?",
    "Ki a PR felelős a dékáni hivatalban?",
    "Ki a tanszékvezető helyettes az alkalmazott nyelvészeti tanszéknek?",
    "Mikor jelent meg a mechatronika képzési szak a romániai felsőoktatásban?",
    "Ki a szoftverfejlesztés szak szakkordinátora?",
    "Hány férőhely van a könyvtárban helyben olvasásra?",
    "Melyek a nem kölcsönözhető dokumentukok a könyvtárból?",
    "Hányas formanyomtatványt kell kitöltenie aki az adója 3,5%-ával a Sapientia Erdélyi Magyar Tudományegyetemet támogatja?",
    "Mi a neve a gazdasági igazgatónak?",
    "Mi az a varjútábor?",
    "Mi a feladatköre Ozsváth-Berényi Attilának?",
    "Hány hektáros területen gyakorlatoznak a kertészmérnöki szakos diákok?",
    "Ki az  Informatika bizottság előke?",
    "Hogyan és mivel tudok Kölcsönözni könyvet?",
    "Mi a HÖK három legfontosabb feladata?",
    "Vizsgaidőszakban mikor van nyitva a könyvtár?",
    " Melyik linken van a Matematika-Informatika tanszék?",
    " Mi az elérhetősége a Marosvasarhelyi Karnak?"
]
correct_answers = [
    "2018-ban egy újabb szárny nyílt meg a Campuson, a C épület, amelyben két amfiteátrum, számos terem és laboratórium is létesült.",
    "dr. Horobeț Emil, egyetemi docens",
    """Vakációban (a nyári szünet alatt is):
    *   hétfő – péntek: 8:00 – 15:00
    *   szombat – vasárnap: zárva.""",
    "Ungvári Zsuzsi, PR-felelős",
    "Dr. Suba Réka - docens, tanszékvezető-helyettes, szakkoordinátor (Fordító és tolmács szak)",
    "A Mechatronika képzési szak 1990 után jelent meg a romániai felsőoktatásban, az ipar által támasztott igények kielégítésére, azon szakterületeken, ahol a szakember gépészmérnöki, villamosmérnöki és informatikai alapismeretekkel kell rendelkezzen.",
    "Dr. Márton Gyöngyvér, adjunktus, szakkoordinátor (Szoftverfejlesztés)",
    "helyben olvasás a 72 férőhelyes szabadpolcos olvasóteremben",
    """További nem kölcsönözhető dokumentumaink:
      *   tájékoztató segédkönyvek (szótárak, lexikonok),
      *   folyóiratok,
      *   szakdolgozatok (raktárból felkérhető).""",
    "Ha az Ön jövedelme csak fizetésből (egy vagy több) származik, akkor a **230**-as formanyomtatványt kell kitöltenie.",
    "Babos Annamária gazdasági igazgató",
    "* A Varjútábort, amit a már elballagott diákoknak szervezünk, hogy együtt tölthessünk egy hétvégét a régi idők emlékére.",
    """Feladatkör: Iroda felelős
    * Név: Ozsváth-Berényi Attila""",
    "A kar főépületét körülvevő 27 hektáros területen a kertészmérnöki szakos diákok gyakorlatoznak, akik parkosítják a terület egy részét.",
    "**Elnök:**\n* dr. Szabó László-Zsolt, egyetemi adjunktus",
    "Kölcsönözni csak személyesen, érvényes könyvtári igazolvánnyal lehet.",
    "A HÖK három legfontosabb feladata:\n* Információ továbbítás\n* Érdekképviselet\n* Rendezvényszervezés",
    "Vizsgaidőszakban szombat délelőttönként is nyitva tartunk, a pontos dátumokat előzőleg hirdetjük a Hírek, közlemények almenüben.",
    "Matematika-Informatika Tanszék (https://ms.sapientia.ro/hu/a-karrol/tanszeke/matematika-informatika-tanszek)",
    """## Marosvásárhelyi Kar
       Târgu-Mureş/Corunca (Marosvásárhely/Koronka), Calea Sighișoarei nr. 2.
       (26, 27, 44-es közszállítási vonal végállomása)
       Tel: +40 265 206 210
       fax: +40 265 206 211
       Postacím: 540485 Târgu-Mureş, O.p. 9, C.p. 4
       E-mail: office@ms.sapientia.ro
       Weboldal (http://ms.sapientia.ro/)"""



]

# Chunkok betöltése
def load_chunks(json_filename):
    with open(json_filename, 'r', encoding='utf-8') as file:
        return json.load(file)

# Top N chunk kiválasztása
def find_top_matches(chunk_data, query_embedding, top_n):
    similarities = []
    for chunk in chunk_data:
        chunk_embedding = np.array(chunk["embedding"])
        similarity = cosine_similarity([query_embedding], [chunk_embedding])[0][0]
        similarities.append((chunk["chunk_id"], chunk["text"], similarity))  # hozzáadtam a chunk_id-t
    similarities.sort(key=lambda x: x[2], reverse=True)  # most a hasonlóság alapján rendezzük
    return similarities[:top_n]

# Válasz értékelése
def check_answer(model_answer, correct_answer,threshold=0.80):
    correct_answer_embeddings = get_embedding(correct_answer)
    model_answer_embedding = get_embedding(model_answer)
    similarity = cosine_similarity(model_answer_embedding.reshape(1, -1), correct_answer_embeddings.reshape(1, -1))[0][0]
    return similarity > threshold, similarity

# Fájl feldolgozás
#json_filenames = ['markdown_karrol_256_.json','markdown_karrol_384_.json','markdown_karrol_512_.json','markdown_karrol_768_.json','markdown_karrol_1024_.json','markdown_karrol_2048_.json']
#json_filenames= ['markdown_karrol_256_small.json','markdown_karrol_384_small.json','markdown_karrol_512_small.json','markdown_karrol_768_small.json','markdown_karrol_1024_small.json','markdown_karrol_2048_small.json']

json_filenames= ['markdown_karrol_256_ada.json','markdown_karrol_384_ada.json','markdown_karrol_512_ada.json','markdown_karrol_768_ada.json','markdown_karrol_1024_ada.json','markdown_karrol_2048_ada.json']
results = []

for json_filename in json_filenames:
    chunk_data = load_chunks(json_filename)
    total_time = 0
    correct_count = 0
    threshold = 0.67
    top_n = 10

    for i, question in enumerate(questions):
        start_time = time.time()
        query_embedding = get_embedding(question)

        # Top találatok lekérése
        top_matches = find_top_matches(chunk_data, query_embedding, top_n=10)
        print("\n--- KÉRDÉS ---")
        print(question)
        print("--- TOP TALÁLATOK ---")
        for j, (chunk_id, text, sim) in enumerate(top_matches, 1):
            print(f"\n{j}. találat (chunk_id: {chunk_id}, hasonlóság: {sim:.4f}):\n{text}")

       # Pontosság ellenőrzés - JAVÍTVA: a legjobb találat szövegét használjuk
        if top_matches:  # Ellenőrizzük, hogy vannak-e találatok
            best_match_text = top_matches[0][1]
            is_correct, similarity = check_answer(best_match_text, correct_answers[i],threshold)  # Helyes válasz indexelése
            if is_correct:
                correct_count += 1
            print(f"\n--- PONTOSSÁG ELLENŐRZÉS ---")
            print(f"Modell válasza (legjobb találat): {best_match_text[:100]}...") # Csak az első 100 karaktert írjuk ki
            print(f"Helyes válasz: {correct_answers[i]}")
            print(f"Hasonlóság a helyes válasszal: {similarity:.4f}, Helyes: {is_correct}")
        else:
            print("\n--- NINCSEN TALÁLAT ---")
        end_time = time.time()
        total_time += (end_time - start_time)



    avg_time = total_time / len(questions)
    accuracy = (correct_count / len(questions)) * 100 if questions else 0


    results.append({
        'model': 'text-embedding-ada-002',
        'Chunk_méret': json_filename,
        'Pontosság(%)': accuracy,
        'Átlagos_feldolgozási_idő': avg_time,
        'Chunk_száma': len(chunk_data),
        'Küszöbérték': threshold,
        'Top_n': top_n

    })

# Eredmények
df = pd.DataFrame(results)
print("\n--- EREDMÉNYEK ---")
print(df)
df.to_excel('results_ada_0_67_threshold_topk_10.xlsx', index=False)  # encoding nem szükséges itt
files.download('results_ada_0_67_threshold_topk_10.xlsx')